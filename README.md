# ğŸ” Semantic Search on arXiv PDFs with OpenSearch & BGE Embeddings

This project sets up a full pipeline for semantic search over scientific papers:

* ğŸ“„ Downloads papers from **arXiv**
* ğŸ§  Extracts text and computes embeddings using **BAAI/bge-base-en** locally
* ğŸ” Indexes both dense and sparse (BM25) fields into **OpenSearch**
* ğŸ” Performs **BM25**, **vector (KNN)**, and **hybrid search** queries
* ğŸ§ª Interactively tested via a Jupyter notebook

---

## ğŸš€ Features

* ğŸ”¨ Local embedding using HuggingFace model (`BAAI/bge-base-en`)
* ğŸ“¦ Dockerized stack for **OpenSearch** and **Dashboards**
* ğŸ§¾ arXiv PDF downloader and text chunker
* âš™ï¸ Fast, semantic indexing with dense vector support
* ğŸ” Hybrid retrieval combining BM25 + vector similarity
* ğŸ§  No external API or LiteLLM dependency â€” all local

---

## ğŸ§± Architecture

```
arXiv PDFs â†’ text chunks â†’ BGE embeddings (locally via HuggingFace)
                     â†“
              Index in OpenSearch
                     â†“
     Query: BM25 / Vector / Hybrid (via notebook)
```

---

## ğŸ“ Project Structure

```
opensearch-arxiv/
â”œâ”€â”€ embedding-api/                # Embedding microservice (optional, unused in CLI)
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ search_examples.ipynb     # BM25, vector, and hybrid queries
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_arxiv_pdfs.py    # Download arXiv PDFs
â”‚   â””â”€â”€ extract_and_index.py      # Chunk, embed, and index
â”œâ”€â”€ data/
â”‚   â””â”€â”€ arxiv_pdfs/               # Downloaded PDFs
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## âš™ï¸ Requirements

* Docker + Docker Compose (for OpenSearch)
* Python 3.11+ (for running scripts locally)
* [`uv`](https://github.com/astral-sh/uv) as the Python package manager
* 8GB+ RAM recommended for local embeddings

---

## ğŸ”§ Setup

### 1. Clone the repository

```bash
git clone https://github.com/andresdigiovanni/opensearch-arxiv.git
cd opensearch-arxiv
```

### 2. Build Docker containers

```bash
make build
```

### 3. Start OpenSearch

```bash
make up
```

This launches:

* OpenSearch at `http://localhost:9200`
* OpenSearch Dashboards at `http://localhost:5601`

---

## ğŸ“¥ Download arXiv PDFs

```bash
make download
```

This downloads a small sample of arXiv PDFs into `data/arxiv_pdfs/`.

> You can edit `scripts/download_arxiv_pdfs.py` to customize topics, queries or paper IDs.

---

## ğŸ§  Embed and Index into OpenSearch

```bash
make index
```

This script will:

* Extract and chunk text from each PDF
* Compute sentence embeddings using `BAAI/bge-base-en`
* Index into OpenSearch with:

  * Sparse field (`text`) for BM25
  * Dense field (`embedding`) for k-NN vector search

---

## ğŸ” Run Search Queries (Notebook)

Start Jupyter and open the notebook:

```bash
jupyter notebook notebooks/search_examples.ipynb
```

Inside you'll find:

* âœ… BM25 search examples
* ğŸ“ k-NN (dense vector) search
* ğŸ” Hybrid queries combining both

---

## ğŸ§° Makefile Commands

```makefile
make build     # Build Docker services
make up        # Start OpenSearch and Dashboards
make down      # Stop all containers
make download  # Download PDFs from arXiv
make index     # Chunk, embed, and index into OpenSearch
```

---

## âŒ What was removed?

Originally, LiteLLM was considered to expose `/v1/embeddings` via an OpenAI-compatible API.
However, this setup now **uses local HuggingFace inference directly**, without needing LiteLLM or external APIs.

---

## ğŸ“š References

* [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)
* [OpenSearch k-NN plugin](https://opensearch.org/docs/latest/search-plugins/knn/)
* [arXiv API](https://arxiv.org/help/api/)

---

## ğŸ‘¤ Author

Developed by [AndrÃ©s Di Giovanni](https://github.com/andresdigiovanni), 2025.
